"paperId","publicationVenue","title","abstract","venue","year","citationCount","fieldsOfStudy","publicationTypes","publicationDate","journal","authors"
"6d2ed9bf1d83c8db1f9cbf92ea2f57ea90ef6839","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","How Powerful are Graph Neural Networks?","Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.","International Conference on Learning Representations","2018","7035","['Computer Science', 'Mathematics']","['JournalArticle']","2018-10-01","{'name': 'ArXiv', 'volume': 'abs/1810.00826'}","[{'authorId': '3360632', 'name': 'Keyulu Xu'}, {'authorId': '48594758', 'name': 'Weihua Hu'}, {'authorId': '1702139', 'name': 'J. Leskovec'}, {'authorId': '2594093', 'name': 'S. Jegelka'}]"
"189b79cda928d58f695cf8323b9ce2196fc22409","{'id': '458166b3-de17-4bf3-bbbb-e53782de2f0f', 'name': 'Nature Biotechnology', 'type': 'journal', 'alternate_names': ['Nat Biotechnol'], 'issn': '1087-0156', 'url': 'http://www.nature.com/nbt/', 'alternate_urls': ['http://www.nature.com/nbt']}","Graph-based genome alignment and genotyping with HISAT2 and HISAT-genotype","","Nature Biotechnology","2019","7802","['Computer Science', 'Medicine']","['JournalArticle']","2019-08-01","{'name': 'Nature Biotechnology', 'pages': '907 - 915', 'volume': '37'}","[{'authorId': '47181672', 'name': 'Daehwan Kim'}, {'authorId': '39011068', 'name': 'Joseph M. Paggi'}, {'authorId': '2115224068', 'name': 'Chanhee Park'}, {'authorId': '104377747', 'name': 'C. Bennett'}, {'authorId': '1744109', 'name': 'S. Salzberg'}]"
"33998aff64ce51df8dee45989cdca4b6b1329ec4","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","Graph Attention Networks","We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","International Conference on Learning Representations","2017","18482","['Computer Science', 'Mathematics']","['JournalArticle']","2017-10-30","{'name': 'ArXiv', 'volume': 'abs/1710.10903'}","[{'authorId': '3444569', 'name': 'Petar Velickovic'}, {'authorId': '7153363', 'name': 'Guillem Cucurull'}, {'authorId': '8742492', 'name': 'Arantxa Casanova'}, {'authorId': '144290131', 'name': 'Adriana Romero'}, {'authorId': '144269589', 'name': 'P. Lio’'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}]"
"cd8a9914d50b0ac63315872530274d158d6aff09","{'id': 'c7bde2ee-6ad5-49c7-9498-a01e46c162eb', 'name': 'Extended Semantic Web Conference', 'type': 'conference', 'alternate_names': ['Eur Semantic Web Conf', 'Ext Semantic Web Conf', 'European Semantic Web Conference', 'ESWC'], 'url': 'https://link.springer.com/conference/esws'}","Modeling Relational Data with Graph Convolutional Networks","","Extended Semantic Web Conference","2017","4498","['Computer Science', 'Mathematics']","['JournalArticle']","2017-03-17","{'pages': '593-607'}","[{'authorId': '8804828', 'name': 'M. Schlichtkrull'}, {'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '2789097', 'name': 'Peter Bloem'}, {'authorId': '9965217', 'name': 'Rianne van den Berg'}, {'authorId': '144889265', 'name': 'Ivan Titov'}, {'authorId': '1678311', 'name': 'M. Welling'}]"
"81a4fd3004df0eb05d6c1cef96ad33d5407820df","{'id': '79c5a18d-0295-432c-aaa5-961d73de6d88', 'name': 'IEEE Transactions on Neural Networks and Learning Systems', 'alternate_names': ['IEEE Trans Neural Netw Learn Syst'], 'issn': '2162-237X', 'url': 'http://ieeexplore.ieee.org/servlet/opac?punumber=5962385', 'alternate_urls': ['https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385']}","A Comprehensive Survey on Graph Neural Networks","Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.","IEEE Transactions on Neural Networks and Learning Systems","2019","7845","['Computer Science', 'Mathematics', 'Medicine']","['JournalArticle', 'Review']","","{'name': 'IEEE Transactions on Neural Networks and Learning Systems', 'pages': '4-24', 'volume': '32'}","[{'authorId': '2109557884', 'name': 'Zonghan Wu'}, {'authorId': '2585415', 'name': 'Shirui Pan'}, {'authorId': '31370754', 'name': 'Fengwen Chen'}, {'authorId': '2062835', 'name': 'Guodong Long'}, {'authorId': '48934799', 'name': 'Chengqi Zhang'}, {'authorId': '144019071', 'name': 'Philip S. Yu'}]"
"63a513832f56addb67be81a2fa399b233f3030fc","{'id': '1901e811-ee72-4b20-8f7e-de08cd395a10', 'name': 'arXiv.org', 'alternate_names': ['ArXiv'], 'issn': '2331-8422', 'url': 'https://arxiv.org'}","Fast Graph Representation Learning with PyTorch Geometric","We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.","arXiv.org","2019","4008","['Computer Science', 'Mathematics']","['JournalArticle']","2019-03-06","{'name': 'ArXiv', 'volume': 'abs/1903.02428'}","[{'authorId': '3410500', 'name': 'Matthias Fey'}, {'authorId': '9572099', 'name': 'J. E. Lenssen'}]"
"36eff562f65125511b5dfab68ce7f7a943c27478","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","Semi-Supervised Classification with Graph Convolutional Networks","We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.","International Conference on Learning Representations","2016","27163","['Mathematics', 'Computer Science']","['JournalArticle']","2016-09-09","{'name': 'ArXiv', 'volume': 'abs/1609.02907'}","[{'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '1678311', 'name': 'M. Welling'}]"
"1ee0abcb8f0afd74d602255d529d7c2a036a8f02","","Graph Theory","","","2016","14768","","['Review']","","","[{'authorId': '2761897', 'name': 'Frank de Zeeuw'}]"
"e1799aaf23c12af6932dc0ef3dfb1638f01413d1","{'id': 'aab03e41-f80d-48b3-89bd-60eeeceafc7d', 'name': 'ACM Transactions on Graphics', 'type': 'journal', 'alternate_names': ['ACM Trans Graph'], 'issn': '0730-0301', 'url': 'http://www.acm.org/tog/', 'alternate_urls': ['http://portal.acm.org/tog', 'https://tog.acm.org/']}","Dynamic Graph CNN for Learning on Point Clouds","Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.","ACM Transactions on Graphics","2018","5730","['Computer Science']","['JournalArticle']","2018-01-24","{'name': 'ACM Transactions on Graphics (TOG)', 'pages': '1 - 12', 'volume': '38'}","[{'authorId': '2118462083', 'name': 'Yue Wang'}, {'authorId': '1409692637', 'name': 'Yongbin Sun'}, {'authorId': '2117940996', 'name': 'Ziwei Liu'}, {'authorId': '1934849', 'name': 'S. Sarma'}, {'authorId': '1732570', 'name': 'M. Bronstein'}, {'authorId': '1932072', 'name': 'J. Solomon'}]"
"3024f58826a5bce3378af94f677e8fb90cbb49e0","{'id': '8dce23a9-44e0-4381-a39e-2acc1edff700', 'name': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'type': 'conference', 'alternate_names': ['International ACM SIGIR Conference on Research and Development in Information Retrieval', 'Int ACM SIGIR Conf Res Dev Inf Retr', 'SIGIR', 'Annu Int ACM SIGIR Conf Res Dev Inf Retr'], 'url': 'http://www.acm.org/sigir/'}","LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation","Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance. In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.","Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","2020","3209","['Computer Science']","['Book', 'JournalArticle', 'Conference']","2020-02-06","{'name': 'Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval'}","[{'authorId': '7792071', 'name': 'Xiangnan He'}, {'authorId': '2068923630', 'name': 'Kuan Deng'}, {'authorId': '2144796537', 'name': 'Xiang Wang'}, {'authorId': '2694924', 'name': 'Yan Li'}, {'authorId': '1699819', 'name': 'Yongdong Zhang'}, {'authorId': '2146059323', 'name': 'Meng Wang'}]"
"ea5dd6a3d8f210d05e53a7b6fa5e16f1b115f693","{'id': '6c35576a-a87d-4dc1-a576-780572d8d0e6', 'name': 'AI Open', 'type': 'journal', 'issn': '2666-6510', 'url': 'https://www.keaipublishing.com/en/journals/ai-open/'}","Graph Neural Networks: A Review of Methods and Applications","","AI Open","2018","5110","['Computer Science', 'Mathematics']","['JournalArticle', 'Review']","2018-12-20","{'name': 'ArXiv', 'volume': 'abs/1812.08434'}","[{'authorId': '48128428', 'name': 'Jie Zhou'}, {'authorId': '52297757', 'name': 'Ganqu Cui'}, {'authorId': '2621696', 'name': 'Zhengyan Zhang'}, {'authorId': '3443627', 'name': 'Cheng Yang'}, {'authorId': '49293587', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]"
"72edcb3788f9c141a3ed28e6d36f75ca4977d27e","{'id': '67f7f831-711a-43c8-8785-1e09005359b5', 'name': 'International Joint Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['Int Jt Conf Artif Intell', 'IJCAI'], 'url': 'http://www.ijcai.org/'}","Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting","Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.","International Joint Conference on Artificial Intelligence","2017","3384","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","2017-09-14","{'name': 'ArXiv', 'volume': 'abs/1709.04875'}","[{'authorId': '46806278', 'name': 'Ting Yu'}, {'authorId': '26379330', 'name': 'Haoteng Yin'}, {'authorId': '1703952', 'name': 'Zhanxing Zhu'}]"
"597bd2e45427563cdf025e53a3239006aa364cfc","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Open Graph Benchmark: Datasets for Machine Learning on Graphs","We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .","Neural Information Processing Systems","2020","2509","['Computer Science', 'Mathematics']","['JournalArticle']","2020-05-02","{'name': 'ArXiv', 'volume': 'abs/2005.00687'}","[{'authorId': '48594758', 'name': 'Weihua Hu'}, {'authorId': '3410500', 'name': 'Matthias Fey'}, {'authorId': '2095762', 'name': 'M. Zitnik'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '40046694', 'name': 'Hongyu Ren'}, {'authorId': '2156641189', 'name': 'Bowen Liu'}, {'authorId': '1754926', 'name': 'Michele Catasta'}, {'authorId': '1702139', 'name': 'J. Leskovec'}]"
"54906484f42e871f7c47bbfe784a358b1448231f","{'id': '1901e811-ee72-4b20-8f7e-de08cd395a10', 'name': 'arXiv.org', 'alternate_names': ['ArXiv'], 'issn': '2331-8422', 'url': 'https://arxiv.org'}","Variational Graph Auto-Encoders","We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.","arXiv.org","2016","3336","['Computer Science', 'Mathematics']","['JournalArticle']","2016-11-21","{'name': 'ArXiv', 'volume': 'abs/1611.07308'}","[{'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '1678311', 'name': 'M. Welling'}]"
"c5f5f179d80a3bf9b4f29750283a87eaca42e91b","{'id': '8dce23a9-44e0-4381-a39e-2acc1edff700', 'name': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'type': 'conference', 'alternate_names': ['International ACM SIGIR Conference on Research and Development in Information Retrieval', 'Int ACM SIGIR Conf Res Dev Inf Retr', 'SIGIR', 'Annu Int ACM SIGIR Conf Res Dev Inf Retr'], 'url': 'http://www.acm.org/sigir/'}","Neural Graph Collaborative Filtering","Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect. In this work, we propose to integrate the user-item interactions - more specifically the bipartite graph structure - into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec [39] and Collaborative Memory Network [5]. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at https://github.com/xiangwang1223/neural_graph_collaborative_filtering.","Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","2019","2723","['Computer Science']","['JournalArticle', 'Book', 'Conference']","2019-05-20","{'name': 'Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval'}","[{'authorId': '98285513', 'name': 'Xiang Wang'}, {'authorId': '7792071', 'name': 'Xiangnan He'}, {'authorId': '2146058320', 'name': 'Meng Wang'}, {'authorId': '2163400298', 'name': 'Fuli Feng'}, {'authorId': '144078686', 'name': 'Tat-Seng Chua'}]"
"7e71eedb078181873a56f2adcfef9dddaeb95602","{'id': 'fc0a208c-acb7-47dc-a0d4-af8190e21d29', 'name': 'International Conference on Machine Learning', 'type': 'conference', 'alternate_names': ['ICML', 'Int Conf Mach Learn'], 'url': 'https://icml.cc/'}","Simplifying Graph Convolutional Networks","Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.","International Conference on Machine Learning","2019","2961","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","2019-02-19","{'pages': '6861-6871'}","[{'authorId': '24277779', 'name': 'Felix Wu'}, {'authorId': '123437034', 'name': 'Tianyi Zhang'}, {'authorId': '3383481', 'name': 'A. Souza'}, {'authorId': '2258547281', 'name': 'Christopher Fifty'}, {'authorId': None, 'name': 'Tao Yu'}, {'authorId': '7446832', 'name': 'Kilian Q. Weinberger'}]"
"6c96c2d4a3fbd572fef2d59cb856521ee1746789","{'id': 'a0edb93b-1e95-4128-a295-6b1659149cef', 'name': 'Knowledge Discovery and Data Mining', 'type': 'conference', 'alternate_names': ['KDD', 'Knowl Discov Data Min'], 'url': 'http://www.acm.org/sigkdd/'}","Graph Convolutional Neural Networks for Web-Scale Recommender Systems","Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.","Knowledge Discovery and Data Mining","2018","3359","['Computer Science', 'Mathematics']","['JournalArticle', 'Book', 'Conference']","2018-06-06","{'name': 'Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining'}","[{'authorId': '83539859', 'name': 'Rex Ying'}, {'authorId': '2933399', 'name': 'Ruining He'}, {'authorId': '2118439896', 'name': 'Kaifeng Chen'}, {'authorId': '50988143', 'name': 'Pong Eksombatchai'}, {'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '1702139', 'name': 'J. Leskovec'}]"
"00b7efbf14a54cced4b9f19e663b70ffbd01324b","{'id': 'e07422f9-c065-40c3-a37b-75e98dce79fe', 'name': 'The Web Conference', 'type': 'conference', 'alternate_names': ['Web Conf', 'WWW'], 'url': 'http://www.iw3c2.org/'}","Heterogeneous Graph Attention Network","Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its meta-path based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.","The Web Conference","2019","2294","['Computer Science']","['JournalArticle', 'Book', 'Conference']","2019-03-18","{'name': 'The World Wide Web Conference'}","[{'authorId': '2118449003', 'name': 'Xiao Wang'}, {'authorId': '51111816', 'name': 'Houye Ji'}, {'authorId': '144123161', 'name': 'C. Shi'}, {'authorId': '1735282', 'name': 'Bai Wang'}, {'authorId': '143738684', 'name': 'Peng Cui'}, {'authorId': '152297693', 'name': 'Philip S. Yu'}, {'authorId': '7873409', 'name': 'Yanfang Ye'}]"
"69906b74ee369cd25522ec432ecbc601a77c9d87","","Spectral graph theory","Spectral graph theory is a vast and expanding area of combinatorics. We start these notes by introducing and motivating classical matrices associated with a graph, and then show how to derive combinatorial properties of a graph from the eigenvalues of these matrices. We then examine more modern results such as polynomial interlacing and high dimensional expanders","Zeta and 𝐿-functions in Number Theory and                     Combinatorics","2019","2436","['Mathematics']","","2019-02-28","{'name': 'Zeta and 𝐿-functions in Number Theory and\n                    Combinatorics'}","[{'authorId': '2285529840', 'name': 'Amol Sahebrao Hinge'}]"
"36652428740cd30d245d55889f01a7fb04a91c93","{'id': 'bdc2e585-4e48-4e36-8af1-6d859763d405', 'name': 'AAAI Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'url': 'http://www.aaai.org/'}","Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning","    Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.   ","AAAI Conference on Artificial Intelligence","2018","2679","['Mathematics', 'Computer Science']","['JournalArticle', 'Conference']","2018-01-22","{'pages': '3538-3545'}","[{'authorId': '35692225', 'name': 'Qimai Li'}, {'authorId': '40592359', 'name': 'Zhichao Han'}, {'authorId': '19195265', 'name': 'Xiao-Ming Wu'}]"
"d08a0eb7024dff5c4fabd58144a38031633d4e1a","{'id': 'c22e7c36-3bfa-43e1-bb7b-edccdea2a780', 'name': 'Journal of machine learning research', 'type': 'journal', 'alternate_names': ['Journal of Machine Learning Research', 'J mach learn res', 'J Mach Learn Res'], 'issn': '1532-4435', 'alternate_issns': ['1533-7928'], 'url': 'http://www.ai.mit.edu/projects/jmlr/', 'alternate_urls': ['http://jmlr.csail.mit.edu/', 'http://www.jmlr.org/', 'http://portal.acm.org/affiliated/jmlr']}","Benchmarking Graph Neural Networks","Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.","Journal of machine learning research","2023","861","['Computer Science', 'Mathematics']","['JournalArticle']","","{'name': 'ArXiv', 'volume': 'abs/2003.00982'}","[{'authorId': '51235219', 'name': 'Vijay Prakash Dwivedi'}, {'authorId': '38009979', 'name': 'Chaitanya K. Joshi'}, {'authorId': '81634721', 'name': 'T. Laurent'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}, {'authorId': '2549032', 'name': 'X. Bresson'}]"
"59cdf849049627e4c30f3bd866e3a7e03e893251","{'id': '74b0478a-292c-4fe3-bb2f-71f438f00cc7', 'name': 'Nature Reviews Neuroscience', 'type': 'journal', 'alternate_names': ['Nat Rev Neurosci'], 'issn': '1471-003X', 'url': 'http://www.nature.com/nrn/', 'alternate_urls': ['http://www.nature.com/nrn/index.html']}","Complex brain networks: graph theoretical analysis of structural and functional systems","","Nature Reviews Neuroscience","2009","10519","['Psychology', 'Medicine']","['Review', 'JournalArticle']","2009-03-01","{'name': 'Nature Reviews Neuroscience', 'pages': '186-198', 'volume': '10'}","[{'authorId': '34217324', 'name': 'E. Bullmore'}, {'authorId': '1694232', 'name': 'O. Sporns'}]"
"967a21a111757d6af7f7a25ca7ea2bdf6d505098","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","Deep Graph Infomax","We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.","International Conference on Learning Representations","2018","2227","['Computer Science', 'Mathematics']","['JournalArticle']","2018-09-27","{'name': 'ArXiv', 'volume': 'abs/1809.10341'}","[{'authorId': '3444569', 'name': 'Petar Velickovic'}, {'authorId': '26958176', 'name': 'W. Fedus'}, {'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '144269589', 'name': 'P. Lio’'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}, {'authorId': '40482726', 'name': 'R. Devon Hjelm'}]"
"3a58efcc4558727cc5c131c44923635da4524f33","{'id': '1901e811-ee72-4b20-8f7e-de08cd395a10', 'name': 'arXiv.org', 'alternate_names': ['ArXiv'], 'issn': '2331-8422', 'url': 'https://arxiv.org'}","Relational inductive biases, deep learning, and graph networks","Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI.  The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between ""hand-engineering"" and ""end-to-end"" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.","arXiv.org","2018","3003","['Mathematics', 'Computer Science']","['JournalArticle', 'Review']","2018-06-04","{'name': 'ArXiv', 'volume': 'abs/1806.01261'}","[{'authorId': '2019153', 'name': 'P. Battaglia'}, {'authorId': '2158860', 'name': 'Jessica B. Hamrick'}, {'authorId': '2603033', 'name': 'V. Bapst'}, {'authorId': '1398105826', 'name': 'Alvaro Sanchez-Gonzalez'}, {'authorId': '3133079', 'name': 'V. Zambaldi'}, {'authorId': '145478807', 'name': 'Mateusz Malinowski'}, {'authorId': '2844530', 'name': 'Andrea Tacchetti'}, {'authorId': '143724694', 'name': 'David Raposo'}, {'authorId': '35030998', 'name': 'Adam Santoro'}, {'authorId': None, 'name': 'Ryan Faulkner'}, {'authorId': '1854385', 'name': 'Çaglar Gülçehre'}, {'authorId': '2107148568', 'name': 'H. F. Song'}, {'authorId': '5055381', 'name': 'A. J. Ballard'}, {'authorId': '2058362', 'name': 'J. Gilmer'}, {'authorId': '35188630', 'name': 'George E. Dahl'}, {'authorId': '40348417', 'name': 'Ashish Vaswani'}, {'authorId': '145254624', 'name': 'Kelsey R. Allen'}, {'authorId': '36942233', 'name': 'C. Nash'}, {'authorId': '2066201331', 'name': 'Victoria Langston'}, {'authorId': '1745899', 'name': 'Chris Dyer'}, {'authorId': '2801204', 'name': 'N. Heess'}, {'authorId': '1688276', 'name': 'D. Wierstra'}, {'authorId': '143967473', 'name': 'Pushmeet Kohli'}, {'authorId': '46378362', 'name': 'M. Botvinick'}, {'authorId': '1689108', 'name': 'O. Vinyals'}, {'authorId': '47002813', 'name': 'Yujia Li'}, {'authorId': '1996134', 'name': 'Razvan Pascanu'}]"
"b07c157e7d40e06a4f2d486b16d5180d8b24acb9","","Algebraic Graph Theory","","Graduate texts in mathematics","2001","9489","['Mathematics', 'Computer Science']","","2001-04-20","{'pages': 'I-XIX, 1-439'}","[{'authorId': '2238617836', 'name': 'Christopher D. Godsil'}, {'authorId': '1688178', 'name': 'G. Royle'}]"
"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","{'id': 'bdc2e585-4e48-4e36-8af1-6d859763d405', 'name': 'AAAI Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'url': 'http://www.aaai.org/'}","ConceptNet 5.5: An Open Multilingual Graph of General Knowledge","    Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.   ","AAAI Conference on Artificial Intelligence","2016","2742","['Computer Science']","['JournalArticle', 'Conference']","2016-12-12","{'pages': '4444-4451'}","[{'authorId': '145696762', 'name': 'R. Speer'}, {'authorId': '2060230787', 'name': 'Joshua Chin'}, {'authorId': '2232845', 'name': 'Catherine Havasi'}]"
"994afdf0db0cb0456f4f76468380822c2f532726","{'id': 'bdc2e585-4e48-4e36-8af1-6d859763d405', 'name': 'AAAI Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'url': 'http://www.aaai.org/'}","Learning Entity and Relation Embeddings for Knowledge Graph Completion","    Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH.   ","AAAI Conference on Artificial Intelligence","2015","3409","['Computer Science']","['JournalArticle', 'Conference']","2015-01-25","{'pages': '2181-2187'}","[{'authorId': '2427350', 'name': 'Yankai Lin'}, {'authorId': '49293587', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '2152797839', 'name': 'Yang Liu'}, {'authorId': '144809121', 'name': 'Xuan Zhu'}]"
"3120324069ec20eed853d3f9bbbceb32e4173b93","","Fast approximate energy minimization via graph cuts","In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function's smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an /spl alpha/-/spl beta/-swap: for a pair of labels /spl alpha/,/spl beta/, this move exchanges the labels between an arbitrary set of pixels labeled a and another arbitrary set labeled /spl beta/. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an /spl alpha/-expansion: for a label a, this move assigns an arbitrary set of pixels the label /spl alpha/. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion.","Proceedings of the Seventh IEEE International Conference on Computer Vision","2001","8228","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","2001-11-01","{'name': 'Proceedings of the Seventh IEEE International Conference on Computer Vision', 'pages': '377-384 vol.1', 'volume': '1'}","[{'authorId': '1692688', 'name': 'Yuri Boykov'}, {'authorId': '1922280', 'name': 'Olga Veksler'}, {'authorId': '2984143', 'name': 'R. Zabih'}]"
"2d867297dfe0d3ce2ed5b1d0f2dff88cac46ee94","","Pregel: a system for large-scale graph processing","Many practical computing problems concern large graphs. Standard examples include the Web graph and various social networks. The scale of these graphs - in some cases billions of vertices, trillions of edges - poses challenges to their efficient processing. In this paper we present a computational model suitable for this task. Programs are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology. This vertex-centric approach is flexible enough to express a broad set of algorithms. The model has been designed for efficient, scalable and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronicity makes reasoning about programs easier. Distribution-related details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.","SIGMOD Conference","2010","3988","['Computer Science']","['JournalArticle', 'Book', 'Conference']","2010-06-06","{'name': 'Proceedings of the 2010 ACM SIGMOD International Conference on Management of data'}","[{'authorId': '1766747', 'name': 'G. Malewicz'}, {'authorId': '2268793', 'name': 'Matthew H. Austern'}, {'authorId': '144211012', 'name': 'Aart J. C. Bik'}, {'authorId': '2381974', 'name': 'James C. Dehnert'}, {'authorId': '40607909', 'name': 'I. Horn'}, {'authorId': '1858799', 'name': 'Naty Leiser'}, {'authorId': '2911719', 'name': 'G. Czajkowski'}]"
"2a9fbca9dc6badbeedc591ad829c5c6e0f950fd6","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Graph Contrastive Learning with Augmentations","Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We first design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at https://github.com/Shen-Lab/GraphCL.","Neural Information Processing Systems","2020","1840","['Computer Science']","['JournalArticle']","2020-10-22","{'name': 'ArXiv', 'volume': 'abs/2010.13902'}","[{'authorId': '89197162', 'name': 'Yuning You'}, {'authorId': '2648459', 'name': 'Tianlong Chen'}, {'authorId': '2003767516', 'name': 'Yongduo Sui'}, {'authorId': '145358498', 'name': 'Ting Chen'}, {'authorId': '2969311', 'name': 'Zhangyang Wang'}, {'authorId': '1705610299', 'name': 'Yang Shen'}]"
"8f096071a09701012c9c279aee2a88143a295935","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space","We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.","International Conference on Learning Representations","2018","1970","['Computer Science', 'Mathematics']","['JournalArticle']","2018-09-27","{'name': 'ArXiv', 'volume': 'abs/1902.10197'}","[{'authorId': '48064856', 'name': 'Zhiqing Sun'}, {'authorId': '123580511', 'name': 'Zhihong Deng'}, {'authorId': '143619007', 'name': 'Jian-Yun Nie'}, {'authorId': '152226504', 'name': 'Jian Tang'}]"
"3efd851140aa28e95221b55fcc5659eea97b172d","{'id': '2ac50919-507e-41c7-93a8-721c4b804757', 'name': 'IEEE Transactions on Neural Networks', 'type': 'journal', 'alternate_names': ['IEEE Trans Neural Netw'], 'issn': '1045-9227', 'alternate_issns': ['1941-0093'], 'url': 'http://ieeexplore.ieee.org/servlet/opac?punumber=72'}","The Graph Neural Network Model","Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.","IEEE Transactions on Neural Networks","2009","7005","['Computer Science', 'Medicine']","['JournalArticle', 'Study']","","{'name': 'IEEE Transactions on Neural Networks', 'pages': '61-80', 'volume': '20'}","[{'authorId': '47260481', 'name': 'F. Scarselli'}, {'authorId': '145467467', 'name': 'M. Gori'}, {'authorId': '1733691', 'name': 'A. Tsoi'}, {'authorId': '1784450', 'name': 'M. Hagenbuchner'}, {'authorId': '3073217', 'name': 'G. Monfardini'}]"
"1976c9eeccc7115d18a04f1e7fb5145db6b96002","","Freebase: a collaboratively created graph database for structuring human knowledge","Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.","SIGMOD Conference","2008","5292","['Computer Science']","['JournalArticle', 'Conference']","2008-06-09","{'pages': '1247-1250'}","[{'authorId': '1742448', 'name': 'K. Bollacker'}, {'authorId': '2065123479', 'name': 'Colin Evans'}, {'authorId': '2990264', 'name': 'Praveen K. Paritosh'}, {'authorId': '1399112633', 'name': 'Tim Sturge'}, {'authorId': '2110748390', 'name': 'Jamie Taylor'}]"
"aeeffe327e6c93e9010c7b1e401caa9113723851","{'id': '939ee07c-6009-43f8-b884-69238b40659e', 'name': 'International Journal of Computer Vision', 'type': 'journal', 'alternate_names': ['Int J Comput Vis'], 'issn': '0920-5691', 'url': 'https://www.springer.com/computer/image+processing/journal/11263', 'alternate_urls': ['https://link.springer.com/journal/11263', 'http://link.springer.com/journal/11263']}","Efficient Graph-Based Image Segmentation","","International Journal of Computer Vision","2004","5789","['Computer Science', 'Mathematics']","['JournalArticle']","2004-09-01","{'name': 'International Journal of Computer Vision', 'pages': '167-181', 'volume': '59'}","[{'authorId': '1685089', 'name': 'Pedro F. Felzenszwalb'}, {'authorId': '1713089', 'name': 'D. Huttenlocher'}]"
"b8d9b10cf54629364523ec065e6307ab87f7d4f0","{'id': '3dbf084c-ef47-4b74-9919-047b40704538', 'name': 'Italian National Conference on Sensors', 'type': 'conference', 'alternate_names': ['SENSORS', 'IEEE Sens', 'Ital National Conf Sens', 'IEEE Sensors', 'Sensors'], 'issn': '1424-8220', 'url': 'http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001', 'alternate_urls': ['http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001', 'http://www.mdpi.com/journal/sensors', 'https://www.mdpi.com/journal/sensors']}","iRun: Horizontal and Vertical Shape of a Region-Based Graph Compression","Graph data are pervasive worldwide, e.g., social networks, citation networks, and web graphs. A real-world graph can be huge and requires heavy computational and storage resources for processing. Various graph compression techniques have been presented to accelerate the processing time and utilize memory efficiently. SOTA approaches decompose a graph into fixed-size submatrices and compress it by applying the existing graph compression algorithm. This approach is promising if the input graph is dense. Otherwise, an optimal graph compression ratio cannot be achieved. Graphs such as those used by social networks exhibit a power-law distribution. Thus, applying compression to the fixed-size block of a matrix could lead to the empty cell processing of that matrix. In this paper, we solve the problem of ordered matrix compression on a deep level, dividing the block into sub-blocks to achieve the best compression ratio. We observe that the ordered matrix compression ratio could be improved by adopting variable-shape regions, considering both horizontal- and vertical-shaped regions. In our empirical evaluation, the proposed approach achieved a 93.8% compression ratio on average, compared with existing SOTA graph compression techniques.","Italian National Conference on Sensors","2022","1389","['Computer Science', 'Medicine']","['JournalArticle']","2022-12-01","{'name': 'Sensors (Basel, Switzerland)', 'volume': '22'}","[{'authorId': '2198082533', 'name': 'Muhammad Umair'}, {'authorId': '2145419030', 'name': 'Young-Koo Lee'}]"
"1d81e7f428fea2b2e15ee3a96fe843ca603acc4c","{'id': 'fc0a208c-acb7-47dc-a0d4-af8190e21d29', 'name': 'International Conference on Machine Learning', 'type': 'conference', 'alternate_names': ['ICML', 'Int Conf Mach Learn'], 'url': 'https://icml.cc/'}","Simple and Deep Graph Convolutional Networks","Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the {\em over-smoothing} problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: {\em Initial residual} and {\em Identity mapping}. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks. Code is available at this https URL .","International Conference on Machine Learning","2020","1363","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","2020-07-04","{'pages': '1725-1735'}","[{'authorId': '2108633355', 'name': 'Ming Chen'}, {'authorId': '12457830', 'name': 'Zhewei Wei'}, {'authorId': '3251920', 'name': 'Zengfeng Huang'}, {'authorId': '1696332', 'name': 'Bolin Ding'}, {'authorId': '2110479359', 'name': 'Yaliang Li'}]"
"2a3f862199883ceff5e3c74126f0c80770653e05","{'id': 'bdc2e585-4e48-4e36-8af1-6d859763d405', 'name': 'AAAI Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'url': 'http://www.aaai.org/'}","Knowledge Graph Embedding by Translating on Hyperplanes","    We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.   ","AAAI Conference on Artificial Intelligence","2014","3415","['Computer Science']","['JournalArticle', 'Conference']","2014-06-21","{'pages': '1112-1119'}","[{'authorId': '2118452539', 'name': 'Zhen Wang'}, {'authorId': '2108090984', 'name': 'Jianwen Zhang'}, {'authorId': '2592554', 'name': 'Jianlin Feng'}, {'authorId': '35773227', 'name': 'Zheng Chen'}]"
"d18b48f77eb5c517a6d2c1fa434d2952a1b0a825","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Hierarchical Graph Representation Learning with Differentiable Pooling","Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets.","Neural Information Processing Systems","2018","2049","['Computer Science', 'Mathematics']","['JournalArticle']","2018-06-22","{'name': 'ArXiv', 'volume': 'abs/1806.08804'}","[{'authorId': '83539859', 'name': 'Rex Ying'}, {'authorId': '145829303', 'name': 'Jiaxuan You'}, {'authorId': '143622465', 'name': 'Christopher Morris'}, {'authorId': '145201124', 'name': 'Xiang Ren'}, {'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '1702139', 'name': 'J. Leskovec'}]"
"6bc77c4dc6075ee81c05f0f5f43e44b2a34a5876","","Graph Theory with Applications","When I first entered the world of Mathematics, I became aware of a strange and little-regarded sect of ""Graph Theorists"", inhabiting a shadowy borderland known to the rest of the community as the ""slums of Topology"". What changes there have been in a few short years! That shadowy borderland has become a thriving metropolis. International conferences on Graph Theory occur with almost embarrassing frequency. Journals on Graph Theory abound: I once counted the Editorial Offices of three of them in one of the mathematical departments of one of the Universities of one of the smaller cities of Canada. Any connection with Topology is likely to be firmly repudiated as soon as noted. I became aware of the burgeoning of Graph Theory when I studied the 1940 paper of Brooks, Smith, Stone and Tutte in the Duke Mathematical Journal, ostensibly on squared rectangles. They wrote of trees and Kirchhoffs Laws, of 3-connection and planarity, of duality and symmetry, of determinantal identities and coprime integers, ~ all in the Quest of the Perfect Square. I invariably recommend that paper to my students. ""Go to it"", I say, ""you will","","1977","5536","['Computer Science']","","","{'name': 'Journal of the Operational Research Society', 'volume': ''}","[{'authorId': '48611572', 'name': 'E. S. Buffa'}]"
"415224a9aff759f6972189df8b7761dfd6a81154","{'id': '992e3995-f295-4778-a913-3fe2f227da3f', 'name': 'Mathematical Gazette', 'type': 'journal', 'alternate_names': ['Math Gaz', 'The Mathematical Gazette'], 'issn': '0025-5572', 'url': 'https://www.cambridge.org/core/journals/mathematical-gazette', 'alternate_urls': ['https://www.m-a.org.uk/resources/periodicals/the_mathematical_gazette/', 'http://journals.cambridge.org/MAG', 'https://www.jstor.org/journal/mathgaze']}","Introduction to graph theory","In graph theory, the term graph refers to a set of vertices and a set of edges. A vertex can be used to represent any object. Graphs may contain undirected or directed edges. An undirected edge is a set of two vertices. A directed edge is an ordered pair of two vertices where the edge goes from the first vertex to the second vertex. Graphs that contain directed edges are called directed graphs or digraphs.","Mathematical Gazette","1973","6800","['Computer Science']","['JournalArticle']","1973-12-01","{'name': 'The Mathematical Gazette', 'pages': '348 - 348', 'volume': '57'}","[{'authorId': '1774907', 'name': 'K. Fraughnaugh'}]"
"9697d32ed0a16da167f2bdba05ef96d0da066eb5","{'id': 'bdc2e585-4e48-4e36-8af1-6d859763d405', 'name': 'AAAI Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'url': 'http://www.aaai.org/'}","Convolutional 2D Knowledge Graph Embeddings","    Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models — which potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree — which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set — however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets — deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets.   ","AAAI Conference on Artificial Intelligence","2017","2454","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","2017-07-05","{'pages': '1811-1818'}","[{'authorId': '3239480', 'name': 'Tim Dettmers'}, {'authorId': '3051815', 'name': 'Pasquale Minervini'}, {'authorId': '1918552', 'name': 'Pontus Stenetorp'}, {'authorId': '48662861', 'name': 'Sebastian Riedel'}]"
"385742fffcf113656f0d3cf6c06ef95cb8439dc6","","Depth-First Search and Linear Graph Algorithms","The value of depth-first search or “backtracking” as a technique for solving problems is illustrated by two examples. An improved version of an algorithm for finding the strongly connected components of a directed graph and at algorithm for finding the biconnected components of an undirect graph are presented. The space and time requirements of both algorithms are bounded by $k_1 V + k_2 E + k_3 $ for some constants $k_1 ,k_2 $, and $k_3 $, where V is the number of vertices and E is the number of edges of the graph being examined.","SIAM journal on computing (Print)","1972","6919","['Mathematics', 'Computer Science']","['JournalArticle']","1972-06-01","{'name': 'SIAM J. Comput.', 'pages': '146-160', 'volume': '1'}","[{'authorId': '1721050', 'name': 'R. Tarjan'}]"
"f412bb31ec9ef8bbef70eefc7ffd04420c1365d9","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Graph-Based Visual Saliency","A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed. It consists of two steps: first forming activation maps on certain feature channels, and then normalizing them in a way which highlights conspicuity and admits combination with other maps. The model is simple, and biologically plausible insofar as it is naturally parallelized. This model powerfully predicts human fixations on 749 variations of 108 natural images, achieving 98% of the ROC area of a human-based control, whereas the classical algorithms of Itti & Koch ([2], [3], [4]) achieve only 84%.","Neural Information Processing Systems","2006","3733","['Computer Science']","['JournalArticle', 'Conference']","2006-12-04","{'pages': '545-552'}","[{'authorId': '39810944', 'name': 'Jonathan Harel'}, {'authorId': '145624227', 'name': 'C. Koch'}, {'authorId': '1690922', 'name': 'P. Perona'}]"
"fae129338c0899576524506008427f64477d3967","{'id': '67f7f831-711a-43c8-8785-1e09005359b5', 'name': 'International Joint Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['Int Jt Conf Artif Intell', 'IJCAI'], 'url': 'http://www.ijcai.org/'}","Graph WaveNet for Deep Spatial-Temporal Graph Modeling","Spatial-temporal graph modeling is an important task to analyze the spatial relations and temporal trends of components in a system. Existing approaches mostly capture the spatial dependency on a fixed graph structure, assuming that the underlying relation between entities is pre-determined. However, the explicit graph structure (relation) does not necessarily reflect the true dependency and genuine relation may be missing due to the incomplete connections in the data. Furthermore, existing methods are ineffective to capture the temporal trends as the RNNs or CNNs employed in these methods cannot capture long-range temporal sequences. To overcome these limitations, we propose in this paper a novel graph neural network architecture, {Graph WaveNet}, for spatial-temporal graph modeling. By developing a novel adaptive dependency matrix and learn it through node embedding, our model can precisely capture the hidden spatial dependency in the data. With a stacked dilated 1D convolution component whose receptive field grows exponentially as the number of layers increases, Graph WaveNet is able to handle very long sequences. These two components are integrated seamlessly in a unified framework and the whole framework is learned in an end-to-end manner. Experimental results on two public traffic network datasets, METR-LA and PEMS-BAY, demonstrate the superior performance of our algorithm.","International Joint Conference on Artificial Intelligence","2019","1917","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","2019-05-31","{'pages': '1907-1913'}","[{'authorId': '2109557884', 'name': 'Zonghan Wu'}, {'authorId': '2585415', 'name': 'Shirui Pan'}, {'authorId': '2062835', 'name': 'Guodong Long'}, {'authorId': '1746594', 'name': 'Jing Jiang'}, {'authorId': '48934799', 'name': 'Chengqi Zhang'}]"
"30321b036607a7936221235ea8ec7cf7c1627100","{'id': 'c6840156-ee10-4d78-8832-7f8909811576', 'name': 'IEEE Transactions on Knowledge and Data Engineering', 'type': 'journal', 'alternate_names': ['IEEE Trans Knowl Data Eng'], 'issn': '1041-4347', 'url': 'https://www.computer.org/web/tkde', 'alternate_urls': ['http://ieeexplore.ieee.org/servlet/opac?punumber=69']}","Knowledge Graph Embedding: A Survey of Approaches and Applications","Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.","IEEE Transactions on Knowledge and Data Engineering","2017","2117","['Computer Science']","['JournalArticle', 'Review']","2017-12-01","{'name': 'IEEE Transactions on Knowledge and Data Engineering', 'pages': '2724-2743', 'volume': '29'}","[{'authorId': '143906199', 'name': 'Quan Wang'}, {'authorId': '1855978', 'name': 'Zhendong Mao'}, {'authorId': '37722675', 'name': 'Bin Wang'}, {'authorId': '48358041', 'name': 'Li Guo'}]"
"de02ec03f6a71246e505862a7195894601fbab99","{'id': 'a0edb93b-1e95-4128-a295-6b1659149cef', 'name': 'Knowledge Discovery and Data Mining', 'type': 'conference', 'alternate_names': ['KDD', 'Knowl Discov Data Min'], 'url': 'http://www.acm.org/sigkdd/'}","KGAT: Knowledge Graph Attention Network for Recommendation","To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism. We release the codes and datasets at https://github.com/xiangwang1223/knowledge_graph_attention_network.","Knowledge Discovery and Data Mining","2019","1697","['Computer Science', 'Mathematics']","['JournalArticle', 'Book', 'Conference']","2019-05-20","{'name': 'Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining'}","[{'authorId': '98285513', 'name': 'Xiang Wang'}, {'authorId': '7792071', 'name': 'Xiangnan He'}, {'authorId': '2112867078', 'name': 'Yixin Cao'}, {'authorId': '2152972434', 'name': 'Meng Liu'}, {'authorId': '144078686', 'name': 'Tat-Seng Chua'}]"
"492f57ee9ceb61fb5a47ad7aebfec1121887a175","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","Gated Graph Sequence Neural Networks","Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.","International Conference on Learning Representations","2015","3181","['Computer Science', 'Mathematics']","['JournalArticle']","2015-11-17","{'name': 'arXiv: Learning', 'volume': ''}","[{'authorId': '47002813', 'name': 'Yujia Li'}, {'authorId': '1725299', 'name': 'Daniel Tarlow'}, {'authorId': '2107692', 'name': 'Marc Brockschmidt'}, {'authorId': '1804104', 'name': 'R. Zemel'}]"
"789a7069d1a2d02d784e4821685b216cc63e6ec8","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","Strategies for Pre-training Graph Neural Networks","Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.","International Conference on Learning Representations","2019","1302","['Computer Science', 'Mathematics']","['JournalArticle']","2019-05-29","{'name': 'arXiv: Learning', 'volume': ''}","[{'authorId': '48594758', 'name': 'Weihua Hu'}, {'authorId': '2156641189', 'name': 'Bowen Liu'}, {'authorId': '145986494', 'name': 'Joseph Gomes'}, {'authorId': '2095762', 'name': 'M. Zitnik'}, {'authorId': '145419642', 'name': 'Percy Liang'}, {'authorId': '1806271', 'name': 'V. Pande'}, {'authorId': '1702139', 'name': 'J. Leskovec'}]"
"8ea9cb53779a8c1bb0e53764f88669bd7edf38f0","{'id': 'fc0a208c-acb7-47dc-a0d4-af8190e21d29', 'name': 'International Conference on Machine Learning', 'type': 'conference', 'alternate_names': ['ICML', 'Int Conf Mach Learn'], 'url': 'https://icml.cc/'}","E(n) Equivariant Graph Neural Networks","This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.","International Conference on Machine Learning","2021","903","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","2021-02-19","{'name': 'ArXiv', 'volume': 'abs/2102.09844'}","[{'authorId': '73240341', 'name': 'Victor Garcia Satorras'}, {'authorId': '65928943', 'name': 'Emiel Hoogeboom'}, {'authorId': '1678311', 'name': 'M. Welling'}]"
"38efda194ce8ca7166119ae403e43aef57f62f7c","","Graph-based Algorithm for Boolean Function Manipulation","","","1989","6495","['Computer Science']","","","{'name': 'IEEE Transactions on Computers', 'volume': ''}","[{'authorId': '34673652', 'name': 'R. Bryant'}]"
"ab30672c8c5e4787f6a5985f26a8f281f0db2fb8","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","How Attentive are Graph Attention Networks?","Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library.","International Conference on Learning Representations","2021","910","['Computer Science']","['JournalArticle']","2021-05-30","{'name': 'ArXiv', 'volume': 'abs/2105.14491'}","[{'authorId': '1720739223', 'name': 'Shaked Brody'}, {'authorId': '47051926', 'name': 'Uri Alon'}, {'authorId': '1743232', 'name': 'Eran Yahav'}]"
"30c15f9be29524e72b9744f8dc14faf2a122d65f","{'id': '25248f80-fe99-48e5-9b8e-9baef3b8e23b', 'name': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'type': 'journal', 'alternate_names': ['IEEE Trans Pattern Anal Mach Intell'], 'issn': '0162-8828', 'url': 'http://www.computer.org/tpami/', 'alternate_urls': ['http://www.computer.org/portal/web/tpami', 'http://ieeexplore.ieee.org/servlet/opac?punumber=34']}","What energy functions can be minimized via graph cuts?","","IEEE Transactions on Pattern Analysis and Machine Intelligence","2002","3666","['Computer Science', 'Medicine']","['Study', 'JournalArticle']","2002-05-28","{'name': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'pages': '147-159', 'volume': '26'}","[{'authorId': '144653004', 'name': 'V. Kolmogorov'}, {'authorId': '2984143', 'name': 'R. Zabih'}]"
"b161c4aaddd2983a9d4d5a240bd5ffa84b36c4e7","{'id': 'a0edb93b-1e95-4128-a295-6b1659149cef', 'name': 'Knowledge Discovery and Data Mining', 'type': 'conference', 'alternate_names': ['KDD', 'Knowl Discov Data Min'], 'url': 'http://www.acm.org/sigkdd/'}","GraphMAE: Self-Supervised Masked Graph Autoencoders","Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning---which heavily relies on structural data augmentation and complicated training strategies---has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE (code is publicly available at https://github.com/THUDM/GraphMAE) that mitigates these issues for generative self-supervised graph learning. Instead of reconstructing structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE---a simple graph autoencoder with our careful designs---can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised learning on graphs.","Knowledge Discovery and Data Mining","2022","476","['Computer Science']","['Book', 'JournalArticle', 'Conference']","2022-05-22","{'name': 'Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining'}","[{'authorId': '2068251467', 'name': 'Zhenyu Hou'}, {'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '83546711', 'name': 'Yukuo Cen'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '38385080', 'name': 'Hongxia Yang'}, {'authorId': '47074042', 'name': 'C. Wang'}, {'authorId': '2148911956', 'name': 'Jie Tang'}]"
"75d82765de0900fed1a7a073415d8f7c625f79e8","","Spectral Graph Theory","Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.","","1996","3509","['Computer Science']","","1996-12-03","{'name': '', 'volume': ''}","[{'authorId': '4231116', 'name': 'F. Chung'}]"
"6017e81c5ede6c38b306a3df9738aeb04baa7619","{'id': 'bdc2e585-4e48-4e36-8af1-6d859763d405', 'name': 'AAAI Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'url': 'http://www.aaai.org/'}","Graph Convolutional Networks for Text Classification","Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.","AAAI Conference on Artificial Intelligence","2018","1737","['Computer Science']","['JournalArticle', 'Conference']","2018-09-15","{'name': 'ArXiv', 'volume': 'abs/1809.05679'}","[{'authorId': '100680875', 'name': 'Liang Yao'}, {'authorId': '145449667', 'name': 'Chengsheng Mao'}, {'authorId': '1683396', 'name': 'Yuan Luo'}]"
"6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da","{'id': 'bdc2e585-4e48-4e36-8af1-6d859763d405', 'name': 'AAAI Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'url': 'http://www.aaai.org/'}","Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks","In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.","AAAI Conference on Artificial Intelligence","2018","1547","['Mathematics', 'Computer Science']","['JournalArticle', 'Conference']","2018-10-04","{'pages': '4602-4609'}","[{'authorId': '143622465', 'name': 'Christopher Morris'}, {'authorId': '8787552', 'name': 'Martin Ritzert'}, {'authorId': '3410500', 'name': 'Matthias Fey'}, {'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '9572099', 'name': 'J. E. Lenssen'}, {'authorId': '3329062', 'name': 'Gaurav Rattan'}, {'authorId': '1744396', 'name': 'Martin Grohe'}]"
"75e924bd79d27a23f3f93d9b1ab62a779505c8d2","{'id': 'a0edb93b-1e95-4128-a295-6b1659149cef', 'name': 'Knowledge Discovery and Data Mining', 'type': 'conference', 'alternate_names': ['KDD', 'Knowl Discov Data Min'], 'url': 'http://www.acm.org/sigkdd/'}","Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks","Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.","Knowledge Discovery and Data Mining","2020","1233","['Computer Science', 'Mathematics']","['JournalArticle', 'Book', 'Conference']","2020-05-24","{'name': 'Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining'}","[{'authorId': '2109557884', 'name': 'Zonghan Wu'}, {'authorId': '2585415', 'name': 'Shirui Pan'}, {'authorId': '2062835', 'name': 'Guodong Long'}, {'authorId': '1746594', 'name': 'Jing Jiang'}, {'authorId': '144950946', 'name': 'Xiaojun Chang'}, {'authorId': '48934799', 'name': 'Chengqi Zhang'}]"
"d81fc968196e06ccafd7ea4c008b13e1cad1be64","{'id': 'bdc2e585-4e48-4e36-8af1-6d859763d405', 'name': 'AAAI Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'url': 'http://www.aaai.org/'}","An End-to-End Deep Learning Architecture for Graph Classification","    Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.   ","AAAI Conference on Artificial Intelligence","2018","1477","['Computer Science']","['JournalArticle', 'Conference']","2018-04-29","{'pages': '4438-4445'}","[{'authorId': '3098251', 'name': 'Muhan Zhang'}, {'authorId': '7217944', 'name': 'Zhicheng Cui'}, {'authorId': '40059761', 'name': 'Marion Neumann'}, {'authorId': '9527255', 'name': 'Yixin Chen'}]"
"5c27487c3e0894b65e976a287e6f8c9aa40f089c","","Face recognition by elastic bunch graph matching","We present a system for recognizing human faces from single images out of a large database containing one image per person. Faces are represented by labeled graphs, based on a Gabor wavelet transform. Image graphs of new faces are extracted by an elastic graph matching process and can be compared by a simple similarity function. The system differs from Lades et al. (1993) in three respects. Phase information is used for accurate node positioning. Object-adapted graphs are used to handle large rotations in depth. Image graph extraction is based on a novel data structure, the bunch graph, which is constructed from a small set of sample image graphs.","Proceedings of International Conference on Image Processing","1997","3857","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","1997-07-01","{'name': 'Proceedings of International Conference on Image Processing', 'pages': '129-132 vol.1', 'volume': '1'}","[{'authorId': '1736245', 'name': 'Laurenz Wiskott'}, {'authorId': '145893752', 'name': 'J. Fellous'}, {'authorId': '1721722', 'name': 'N. Krüger'}, {'authorId': '1704573', 'name': 'C. Malsburg'}]"
"eea094ba215c5a6b1b178fd3190f4e10b5d5ca98","{'id': '3da56ce6-4a70-402c-a8b0-7dd39a3273d3', 'name': 'Software, Practice & Experience', 'type': 'journal', 'alternate_names': ['Softw  Pract Exp', 'Softw Pract  Exp', 'Software - Practice and Experience'], 'issn': '0038-0644', 'url': 'http://www3.interscience.wiley.com/cgi-bin/jhome/1752', 'alternate_urls': ['http://www.interscience.wiley.com/jpages/0038-0644/', 'https://onlinelibrary.wiley.com/journal/1097024X']}","Graph drawing by force‐directed placement","We present a modification of the spring‐embedder model of Eades [Congressus Numerantium, 42, 149–160, (1984)] for drawing undirected graphs with straight edges. Our heuristic strives for uniform edge lengths, and we develop it in analogy to forces in natural systems, for a simple, elegant, conceptually‐intuitive, and efficient algorithm.","Software, Practice & Experience","1991","6168","['Computer Science']","['JournalArticle']","1991-11-01","{'name': 'Software: Practice and Experience', 'volume': '21'}","[{'authorId': '38255588', 'name': 'Thomas M. J. Fruchterman'}, {'authorId': '1743464', 'name': 'E. Reingold'}]"
"3d846cb01f6a975554035d2210b578ca61344b22","{'id': 'fc0a208c-acb7-47dc-a0d4-af8190e21d29', 'name': 'International Conference on Machine Learning', 'type': 'conference', 'alternate_names': ['ICML', 'Int Conf Mach Learn'], 'url': 'https://icml.cc/'}","Revisiting Semi-Supervised Learning with Graph Embeddings","We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.","International Conference on Machine Learning","2016","1981","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","2016-03-29","{'name': 'ArXiv', 'volume': 'abs/1603.08861'}","[{'authorId': '2109512754', 'name': 'Zhilin Yang'}, {'authorId': '50056360', 'name': 'William W. Cohen'}, {'authorId': '145124475', 'name': 'R. Salakhutdinov'}]"
"e48f36aacb72adb74cef077c87d2351121124137","{'id': '768b87bb-8a18-4d9c-a161-4d483c776bcf', 'name': 'Computer Vision and Pattern Recognition', 'type': 'conference', 'alternate_names': ['CVPR', 'Comput Vis Pattern Recognit'], 'issn': '1063-6919', 'url': 'https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147', 'alternate_urls': ['https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition']}","Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition","In skeleton-based action recognition, graph convolutional networks (GCNs), which model the human body skeletons as spatiotemporal graphs, have achieved remarkable performance. However, in existing GCN-based methods, the topology of the graph is set manually, and it is fixed over all layers and input samples. This may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks. In addition, the second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods. In this work, we propose a novel two-stream adaptive graph convolutional network (2s-AGCN) for skeleton-based action recognition. The topology of the graph in our model can be either uniformly or individually learned by the BP algorithm in an end-to-end manner. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Moreover, a two-stream framework is proposed to model both the first-order and the second-order information simultaneously, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.","Computer Vision and Pattern Recognition","2018","1334","['Computer Science']","['JournalArticle', 'Conference']","2018-05-20","{'name': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)', 'pages': '12018-12027'}","[{'authorId': '19284184', 'name': 'Lei Shi'}, {'authorId': '40382978', 'name': 'Yifan Zhang'}, {'authorId': '143949499', 'name': 'Jian Cheng'}, {'authorId': '1694235', 'name': 'Hanqing Lu'}]"
"3aca80d2a6ec2014342c4abe6611d498c789f7fa","{'id': '1620da87-4387-4b9a-9bf4-22fdf74d4dc3', 'name': 'Symmetry', 'type': 'journal', 'issn': '2073-8994', 'url': 'http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-172134', 'alternate_urls': ['https://www.mdpi.com/journal/symmetry', 'http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-172134']}","Graph Theory","Abstract","Symmetry","2018","2212","['Computer Science']","['JournalArticle']","2018-10-08","{'name': 'Symmetry', 'pages': '32', 'volume': '10'}","[{'authorId': '2271959477', 'name': 'José M. Rodríguez'}]"
"bea139a35ffe458d7aec576b5e651cd8ac80e4d2","","Introduction to Graph Theory","1. Fundamental Concepts. What Is a Graph? Paths, Cycles, and Trails. Vertex Degrees and Counting. Directed Graphs. 2. Trees and Distance. Basic Properties. Spanning Trees and Enumeration. Optimization and Trees. 3. Matchings and Factors. Matchings and Covers. Algorithms and Applications. Matchings in General Graphs. 4. Connectivity and Paths. Cuts and Connectivity. k-connected Graphs. Network Flow Problems. 5. Coloring of Graphs. Vertex Colorings and Upper Bounds. Structure of k-chromatic Graphs. Enumerative Aspects. 6. Planar Graphs. Embeddings and Euler's Formula. Characterization of Planar Graphs. Parameters of Planarity. 7. Edges and Cycles. Line Graphs and Edge-Coloring. Hamiltonian Cycles. Planarity, Coloring, and Cycles. 8. Additional Topics (Optional). Perfect Graphs. Matroids. Ramsey Theory. More Extremal Problems. Random Graphs. Eigenvalues of Graphs. Appendix A: Mathematical Background. Appendix B: Optimization and Complexity. Appendix C: Hints for Selected Exercises. Appendix D: Glossary of Terms. Appendix E: Supplemental Reading. Appendix F: References. Indices.","","1995","4032","['Mathematics']","","1995-11-14","{'name': '', 'volume': ''}","[{'authorId': '1709358', 'name': 'D. West'}]"
"00358a3f17821476d93461192b9229fe7d92bb3f","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","GNNExplainer: Generating Explanations for Graph Neural Networks","Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GnnExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GnnExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GnnExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GnnExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0% in explanation accuracy. GnnExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.","Neural Information Processing Systems","2019","1182","['Computer Science', 'Medicine', 'Mathematics']","['JournalArticle']","2019-03-10","{'name': 'Advances in neural information processing systems', 'pages': '\n          9240-9251\n        ', 'volume': '32'}","[{'authorId': '83539859', 'name': 'Rex Ying'}, {'authorId': '40974349', 'name': 'Dylan Bourgeois'}, {'authorId': '145829303', 'name': 'Jiaxuan You'}, {'authorId': '2095762', 'name': 'M. Zitnik'}, {'authorId': '1702139', 'name': 'J. Leskovec'}]"
"0ca7d8c3250d43d14fdde46bf6fc299654d861ef","{'id': 'e07422f9-c065-40c3-a37b-75e98dce79fe', 'name': 'The Web Conference', 'type': 'conference', 'alternate_names': ['Web Conf', 'WWW'], 'url': 'http://www.iw3c2.org/'}","Heterogeneous Graph Transformer","Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making it infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9–21 on various downstream tasks. The dataset and source code of HGT are publicly available at https://github.com/acbull/pyHGT.","The Web Conference","2020","1102","['Computer Science', 'Mathematics']","['JournalArticle', 'Book', 'Conference']","2020-03-03","{'name': 'Proceedings of The Web Conference 2020'}","[{'authorId': '3407296', 'name': 'Ziniu Hu'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '1748169', 'name': 'Kuansan Wang'}, {'authorId': '2109461904', 'name': 'Yizhou Sun'}]"
"acf87283fa8ae426f1a4987b345b401bf2913f61","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Do Transformers Really Perform Badly for Graph Representation?","The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer. The code and models of Graphormer will be made publicly available at https://github.com/Microsoft/Graphormer .","Neural Information Processing Systems","2021","752","['Computer Science']","['JournalArticle']","","{'pages': '28877-28888'}","[{'authorId': '2051552141', 'name': 'Chengxuan Ying'}, {'authorId': '123970124', 'name': 'Tianle Cai'}, {'authorId': '2108801920', 'name': 'Shengjie Luo'}, {'authorId': '150311931', 'name': 'Shuxin Zheng'}, {'authorId': '35286545', 'name': 'Guolin Ke'}, {'authorId': '2266036459', 'name': 'Di He'}, {'authorId': '2266126249', 'name': 'Yanming Shen'}, {'authorId': '2266182896', 'name': 'Tie-Yan Liu'}]"
"05c4eb154ad9512a69569c18d68bc4428ee8bb83","{'id': 'a0edb93b-1e95-4128-a295-6b1659149cef', 'name': 'Knowledge Discovery and Data Mining', 'type': 'conference', 'alternate_names': ['KDD', 'Knowl Discov Data Min'], 'url': 'http://www.acm.org/sigkdd/'}","Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks","Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy---using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by~\citezhang2018gaan.","Knowledge Discovery and Data Mining","2019","1210","['Computer Science', 'Mathematics']","['Book', 'JournalArticle', 'Conference']","2019-05-20","{'name': 'Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining'}","[{'authorId': '2537924', 'name': 'Wei-Lin Chiang'}, {'authorId': '23979212', 'name': 'Xuanqing Liu'}, {'authorId': '3422911', 'name': 'Si Si'}, {'authorId': '1678662', 'name': 'Yang Li'}, {'authorId': '1751569', 'name': 'Samy Bengio'}, {'authorId': '1793529', 'name': 'Cho-Jui Hsieh'}]"
"b5b717bd65678b9b980ae3f2aaea0396e87ab0e2","{'id': '7654260e-79f9-45c5-9663-d72027cf88f3', 'name': 'IEEE International Conference on Computer Vision', 'type': 'conference', 'alternate_names': ['ICCV', 'IEEE Int Conf Comput Vis', 'ICCV Workshops', 'ICCV Work'], 'url': 'https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings'}","Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition","Graph convolutional networks (GCNs) have been widely used and achieved remarkable results in skeleton-based action recognition. In GCNs, graph topology dominates feature aggregation and therefore is the key to extracting representative features. In this work, we propose a novel Channel-wise Topology Refinement Graph Convolution (CTR-GC) to dynamically learn different topologies and effectively aggregate joint features in different channels for skeleton-based action recognition. The proposed CTR-GC models channel-wise topologies through learning a shared topology as a generic prior for all channels and refining it with channel-specific correlations for each channel. Our refinement method introduces few extra parameters and significantly reduces the difficulty of modeling channel-wise topologies. Furthermore, via reformulating graph convolutions into a unified form, we find that CTR-GC relaxes strict constraints of graph convolutions, leading to stronger representation capability. Combining CTR-GC with temporal modeling modules, we develop a powerful graph convolutional network named CTR-GCN which notably outperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.1","IEEE International Conference on Computer Vision","2021","506","['Computer Science']","['JournalArticle', 'Conference']","2021-07-26","{'name': '2021 IEEE/CVF International Conference on Computer Vision (ICCV)', 'pages': '13339-13348'}","[{'authorId': '2137389378', 'name': 'Yuxin Chen'}, {'authorId': '2144370824', 'name': 'Ziqi Zhang'}, {'authorId': '50199457', 'name': 'Chunfen Yuan'}, {'authorId': '2152691925', 'name': 'Bing Li'}, {'authorId': '2115656688', 'name': 'Ying Deng'}, {'authorId': '40506509', 'name': 'Weiming Hu'}]"
"398d6f4432e6aa7acf21c0bbaaebac48998faad3","{'id': 'e07422f9-c065-40c3-a37b-75e98dce79fe', 'name': 'The Web Conference', 'type': 'conference', 'alternate_names': ['Web Conf', 'WWW'], 'url': 'http://www.iw3c2.org/'}","Graph Neural Networks for Social Recommendation","In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.","The Web Conference","2019","1754","['Computer Science']","['JournalArticle', 'Book', 'Conference']","2019-02-19","{'name': 'The World Wide Web Conference'}","[{'authorId': '41031455', 'name': 'Wenqi Fan'}, {'authorId': '47009435', 'name': 'Yao Ma'}, {'authorId': '1930238', 'name': 'Qing Li'}, {'authorId': '2145051005', 'name': 'Yuan He'}, {'authorId': '2109917536', 'name': 'Y. Zhao'}, {'authorId': '1736632', 'name': 'Jiliang Tang'}, {'authorId': '50559722', 'name': 'Dawei Yin'}]"
"fd17bd9a5dc24a081ad9743570f50dd6750f54b2","{'id': 'fc0a208c-acb7-47dc-a0d4-af8190e21d29', 'name': 'International Conference on Machine Learning', 'type': 'conference', 'alternate_names': ['ICML', 'Int Conf Mach Learn'], 'url': 'https://icml.cc/'}","Junction Tree Variational Autoencoder for Molecular Graph Generation","We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.","International Conference on Machine Learning","2018","1287","['Computer Science', 'Mathematics']","['JournalArticle', 'Conference']","2018-02-12","{'name': 'ArXiv', 'volume': 'abs/1802.04364'}","[{'authorId': '2400119', 'name': 'Wengong Jin'}, {'authorId': '1741283', 'name': 'R. Barzilay'}, {'authorId': '35132120', 'name': 'T. Jaakkola'}]"
"2503dff90685857ce7295e37d0045e2eef41c8b8","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling","The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.","International Conference on Learning Representations","2018","1453","['Computer Science', 'Mathematics']","['JournalArticle']","2018-01-30","{'name': 'ArXiv', 'volume': 'abs/1801.10247'}","[{'authorId': None, 'name': 'Jie Chen'}, {'authorId': '40411766', 'name': 'Tengfei Ma'}, {'authorId': '145781464', 'name': 'Cao Xiao'}]"
"e4715a13f6364b1c81e64f247651c3d9e80b6808","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Link Prediction Based on Graph Neural Networks","Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.","Neural Information Processing Systems","2018","1810","['Computer Science', 'Mathematics']","['JournalArticle']","2018-02-27","{'pages': '5171-5181'}","[{'authorId': '3098251', 'name': 'Muhan Zhang'}, {'authorId': '9527255', 'name': 'Yixin Chen'}]"
"348cd9726be5e5740ab751c15fbad4b60d98246d","","Patterns in Hydrogen Bonding: Functionality and Graph Set Analysis in Crystals","Whereas much of organic chemistry has classically dealt with the preparation and study of the properties of individual molecules, an increasingly significant portion of the activity in chemical research involves understanding and utilizing the nature of the interactions between molecules. Two representative areas of this evolution are supramolecular chemistry and molecular recognition. The interactions between molecules are governed by intermolecular forces whose energetic and geometric properties are much less well understood than those of classical chemical bonds between atoms. Among the strongest of these interactions, however, are hydrogen bonds, whose directional properties are better understood on the local level (that is, for a single hydrogen bond) than many other types of non-bonded interactions. Nevertheless, the means by which to characterize, understand, and predict the consequences of many hydrogen bonds among molecules, and the resulting formation of molecular aggregates (on the microscopic scale) or crystals (on the macroscopic scale) has remained largely enigmatic. One of the most promising systematic approaches to resolving this enigma was initially developed by the late M. C. Etter, who applied graph theory to recognize, and then utilize, patterns of hydrogen bonding for the understanding and design of molecular crystals. In working with Etter's original ideas the power and potential utility of this approach on one hand, and on the other, the need to develop and extend the initial Etter formalism was generally recognized. It with that latter purpose that we originally undertook the present review.","","1995","7386","['Chemistry']","['Review']","1995-08-18","{'name': 'Angewandte Chemie', 'pages': '1555-1573', 'volume': '34'}","[{'authorId': '144302936', 'name': 'J. Bernstein'}, {'authorId': '48778365', 'name': 'Raymond E. Davis'}, {'authorId': '2308579', 'name': 'L. Shimoni'}, {'authorId': '143976370', 'name': 'N. Chang'}]"
"04f3203f1214063436d81ce0c2ad7623204da488","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","Geom-GCN: Geometric Graph Convolutional Networks","Message-passing neural networks (MPNNs) have been successfully applied in a wide variety of applications in the real world. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN, to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs.","International Conference on Learning Representations","2020","999","['Computer Science', 'Mathematics']","['JournalArticle']","2020-02-13","{'name': 'ArXiv', 'volume': 'abs/2002.05287'}","[{'authorId': '1866075', 'name': 'Hongbin Pei'}, {'authorId': '15763974', 'name': 'Bingzhen Wei'}, {'authorId': '143922493', 'name': 'K. Chang'}, {'authorId': '2113651237', 'name': 'Yu Lei'}, {'authorId': '2119658599', 'name': 'Bo Yang'}]"
"21e33bd0ad95ee1f79d8b778e693fd316cbb72d4","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs","We investigate the representation power of graph neural networks in the semi-supervised node classification task under heterophily or low homophily, i.e., in networks where connected nodes may have different class labels and dissimilar features. Many popular GNNs fail to generalize to this setting, and are even outperformed by models that ignore the graph structure (e.g., multilayer perceptrons). Motivated by this limitation, we identify a set of key designs -- ego- and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations -- that boost learning from the graph structure under heterophily. We combine them into a graph neural network, H2GCN, which we use as the base method to empirically evaluate the effectiveness of the identified designs. Going beyond the traditional benchmarks with strong homophily, our empirical analysis shows that the identified designs increase the accuracy of GNNs by up to 40% and 27% over models without them on synthetic and real networks with heterophily, respectively, and yield competitive performance under homophily.","Neural Information Processing Systems","2020","891","['Computer Science']","['JournalArticle']","2020-06-20","{'name': 'arXiv: Learning', 'volume': ''}","[{'authorId': '50077183', 'name': 'Jiong Zhu'}, {'authorId': '7957569', 'name': 'Yujun Yan'}, {'authorId': '21613538', 'name': 'Lingxiao Zhao'}, {'authorId': '35505461', 'name': 'Mark Heimann'}, {'authorId': '3255268', 'name': 'L. Akoglu'}, {'authorId': '2479152', 'name': 'Danai Koutra'}]"
"699d4d8c8b91b1a34dd6f47d28e3fe1236f25944","","Saliency Detection via Graph-Based Manifold Ranking","Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with super pixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult benchmark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.","2013 IEEE Conference on Computer Vision and Pattern Recognition","2013","2281","['Computer Science']","['JournalArticle', 'Conference']","2013-06-23","{'name': '2013 IEEE Conference on Computer Vision and Pattern Recognition', 'pages': '3166-3173'}","[{'authorId': '2154926843', 'name': 'Chuan Yang'}, {'authorId': '50081215', 'name': 'L. Zhang'}, {'authorId': '153176123', 'name': 'Huchuan Lu'}, {'authorId': '144526777', 'name': 'Xiang Ruan'}, {'authorId': '1715634', 'name': 'Ming-Hsuan Yang'}]"
"c2d40522eaa5523d67a0de5e4098e7031fdccb3d","{'id': '1901e811-ee72-4b20-8f7e-de08cd395a10', 'name': 'arXiv.org', 'alternate_names': ['ArXiv'], 'issn': '2331-8422', 'url': 'https://arxiv.org'}","Pitfalls of Graph Neural Network Evaluation","Semi-supervised node classification in graphs is a fundamental problem in graph mining, and the recently proposed graph neural networks (GNNs) have achieved unparalleled results on this task. Due to their massive success, GNNs have attracted a lot of attention, and many novel architectures have been put forward. In this paper we show that existing evaluation strategies for GNN models have serious shortcomings. We show that using the same train/validation/test splits of the same datasets, as well as making significant changes to the training procedure (e.g. early stopping criteria) precludes a fair comparison of different architectures. We perform a thorough empirical evaluation of four prominent GNN models and show that considering different splits of the data leads to dramatically different rankings of models. Even more importantly, our findings suggest that simpler GNN architectures are able to outperform the more sophisticated ones if the hyperparameters and the training procedure are tuned fairly for all models.","arXiv.org","2018","1255","['Computer Science', 'Mathematics']","['JournalArticle']","2018-11-14","{'name': 'ArXiv', 'volume': 'abs/1811.05868'}","[{'authorId': '32724677', 'name': 'Oleksandr Shchur'}, {'authorId': '51896688', 'name': 'Maximilian Mumme'}, {'authorId': '11754930', 'name': 'Aleksandar Bojchevski'}, {'authorId': '3075189', 'name': 'Stephan Günnemann'}]"
"75739ed2ddebd7982042f516f407553f8d3110f8","{'id': '8dce23a9-44e0-4381-a39e-2acc1edff700', 'name': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'type': 'conference', 'alternate_names': ['International ACM SIGIR Conference on Research and Development in Information Retrieval', 'Int ACM SIGIR Conf Res Dev Inf Retr', 'SIGIR', 'Annu Int ACM SIGIR Conf Res Dev Inf Retr'], 'url': 'http://www.acm.org/sigir/'}","Self-supervised Graph Learning for Recommendation","Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges. In this work, we explore self-supervised learning on user-item graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary self-supervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise three operators to generate the views --- node dropout, edge dropout, and random walk --- that change the graph structure in different manners. We term this new learning paradigm asSelf-supervised Graph Learning (SGL), implementing it on the state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL has the ability of automatically mining hard negatives. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the recommendation accuracy, especially on long-tail items, and the robustness against interaction noises. Our implementations are available at \urlhttps://github.com/wujcan/SGL.","Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","2020","1015","['Computer Science']","['JournalArticle', 'Book', 'Conference']","2020-10-21","{'name': 'Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval'}","[{'authorId': '1491035012', 'name': 'Jiancan Wu'}, {'authorId': '2144796537', 'name': 'Xiang Wang'}, {'authorId': '2163400298', 'name': 'Fuli Feng'}, {'authorId': '7792071', 'name': 'Xiangnan He'}, {'authorId': '1853048147', 'name': 'Liang Chen'}, {'authorId': '2813328', 'name': 'Jianxun Lian'}, {'authorId': '2110972323', 'name': 'Xing Xie'}]"
"e3d662bbd0e5539fe22a85f3518f960595b9914e","{'id': 'a0edb93b-1e95-4128-a295-6b1659149cef', 'name': 'Knowledge Discovery and Data Mining', 'type': 'conference', 'alternate_names': ['KDD', 'Knowl Discov Data Min'], 'url': 'http://www.acm.org/sigkdd/'}","Heterogeneous Graph Neural Network","Representation learning in heterogeneous graphs aims to pursue a meaningful vector representation for each node so as to facilitate downstream applications such as link prediction, personalized recommendation, node classification, etc. This task, however, is challenging not only because of the demand to incorporate heterogeneous structural (graph) information consisting of multiple types of nodes and edges, but also due to the need for considering heterogeneous attributes or contents (e.g., text or image) associated with each node. Despite a substantial amount of effort has been made to homogeneous (or heterogeneous) graph embedding, attributed graph embedding as well as graph neural networks, few of them can jointly consider heterogeneous structural (graph) information as well as heterogeneous contents information of each node effectively. In this paper, we propose HetGNN, a heterogeneous graph neural network model, to resolve this issue. Specifically, we first introduce a random walk with restart strategy to sample a fixed size of strongly correlated heterogeneous neighbors for each node and group them based upon node types. Next, we design a neural network architecture with two modules to aggregate feature information of those sampled neighboring nodes. The first module encodes ""deep"" feature interactions of heterogeneous contents and generates content embedding for each node. The second module aggregates content (attribute) embeddings of different neighboring groups (types) and further combines them by considering the impacts of different groups to obtain the ultimate node embedding. Finally, we leverage a graph context loss and a mini-batch gradient descent procedure to train the model in an end-to-end manner. Extensive experiments on several datasets demonstrate that HetGNN can outperform state-of-the-art baselines in various graph mining tasks, i.e., link prediction, recommendation, node classification & clustering and inductive node classification & clustering.","Knowledge Discovery and Data Mining","2019","1219","['Computer Science']","['JournalArticle', 'Book', 'Conference']","2019-07-25","{'name': 'Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining'}","[{'authorId': '3407809', 'name': 'Chuxu Zhang'}, {'authorId': '2451800', 'name': 'Dongjin Song'}, {'authorId': '144277661', 'name': 'Chao Huang'}, {'authorId': '144231976', 'name': 'A. Swami'}, {'authorId': '144539424', 'name': 'N. Chawla'}]"
"44fca068eecce2203d111213e3691647914a3945","{'id': 'aef12dca-60a0-4ca3-819b-cad26d309d4e', 'name': 'Journal of Artificial Intelligence Research', 'type': 'journal', 'alternate_names': ['JAIR', 'J Artif Intell Res', 'The Journal of Artificial Intelligence Research'], 'issn': '1076-9757', 'url': 'http://www.jair.org/'}","LexRank: Graph-based Lexical Centrality as Salience in Text Summarization","We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.","Journal of Artificial Intelligence Research","2004","3041","['Mathematics', 'Computer Science']","['JournalArticle']","2004-07-01","{'name': 'ArXiv', 'volume': 'abs/1109.2128'}","[{'authorId': '2158159', 'name': 'Günes Erkan'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}]"
"5863d7b35ea317c19f707376978ef1cc53e3534c","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Rethinking Graph Transformers with Spectral Attention","In recent years, the Transformer architecture has proven to be very successful in sequence processing, but its application to other data structures, such as graphs, has remained limited due to the difficulty of properly defining positions. Here, we present the $\textit{Spectral Attention Network}$ (SAN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a wide margin, becoming the first fully-connected architecture to perform well on graph benchmarks.","Neural Information Processing Systems","2021","465","['Computer Science']","['JournalArticle']","2021-06-07","{'name': 'ArXiv', 'volume': 'abs/2106.03893'}","[{'authorId': '2107709516', 'name': 'Devin Kreuzer'}, {'authorId': '51034451', 'name': 'D. Beaini'}, {'authorId': '2057555930', 'name': 'William L. Hamilton'}, {'authorId': '2067158294', 'name': ""Vincent L'etourneau""}, {'authorId': '12611623', 'name': 'Prudencio Tossou'}]"
"47ae807cd511b35e78a2cd4e198283dea6dafd41","{'id': '1901e811-ee72-4b20-8f7e-de08cd395a10', 'name': 'arXiv.org', 'alternate_names': ['ArXiv'], 'issn': '2331-8422', 'url': 'https://arxiv.org'}","Do Transformers Really Perform Bad for Graph Representation?","The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.","arXiv.org","2021","426","['Computer Science']","['JournalArticle']","2021-06-09","{'name': 'ArXiv', 'volume': 'abs/2106.05234'}","[{'authorId': '2051552141', 'name': 'Chengxuan Ying'}, {'authorId': '123970124', 'name': 'Tianle Cai'}, {'authorId': '2108801920', 'name': 'Shengjie Luo'}, {'authorId': '150311931', 'name': 'Shuxin Zheng'}, {'authorId': '35286545', 'name': 'Guolin Ke'}, {'authorId': '1391126980', 'name': 'Di He'}, {'authorId': '2115437382', 'name': 'Yanming Shen'}, {'authorId': '2110264337', 'name': 'Tie-Yan Liu'}]"
"06c9cbf943185d1d1554e760279412598c6d81a6","{'id': '8dce23a9-44e0-4381-a39e-2acc1edff700', 'name': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'type': 'conference', 'alternate_names': ['International ACM SIGIR Conference on Research and Development in Information Retrieval', 'Int ACM SIGIR Conf Res Dev Inf Retr', 'SIGIR', 'Annu Int ACM SIGIR Conf Res Dev Inf Retr'], 'url': 'http://www.acm.org/sigir/'}","Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation","Contrastive learning (CL) recently has spurred a fruitful line of research in the field of recommendation, since its ability to extract self-supervised signals from the raw data is well-aligned with recommender systems' needs for tackling the data sparsity issue. A typical pipeline of CL-based recommendation models is first augmenting the user-item bipartite graph with structure perturbations, and then maximizing the node representation consistency between different graph augmentations. Although this paradigm turns out to be effective, what underlies the performance gains is still a mystery. In this paper, we first experimentally disclose that, in CL-based recommendation models, CL operates by learning more uniform user/item representations that can implicitly mitigate the popularity bias. Meanwhile, we reveal that the graph augmentations, which used to be considered necessary, just play a trivial role. Based on this finding, we propose a simple CL method which discards the graph augmentations and instead adds uniform noises to the embedding space for creating contrastive views. A comprehensive experimental study on three benchmark datasets demonstrates that, though it appears strikingly simple, the proposed method can smoothly adjust the uniformity of learned representations and has distinct advantages over its graph augmentation-based counterparts in terms of recommendation accuracy and training efficiency. The code is released at https://github.com/Coder-Yu/QRec.","Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","2021","510","['Computer Science']","['Book', 'JournalArticle', 'Conference']","2021-12-16","{'name': 'Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval'}","[{'authorId': '28584977', 'name': 'Junliang Yu'}, {'authorId': '2416851', 'name': 'Hongzhi Yin'}, {'authorId': '2077454936', 'name': 'Xin Xia'}, {'authorId': '1490931831', 'name': 'Tong Chen'}, {'authorId': '101457473', 'name': 'Li-zhen Cui'}, {'authorId': '144133815', 'name': 'Q. Nguyen'}]"
"4bf76588122827157c43a59e656dccc6b6a22e90","{'id': '1901e811-ee72-4b20-8f7e-de08cd395a10', 'name': 'arXiv.org', 'alternate_names': ['ArXiv'], 'issn': '2331-8422', 'url': 'https://arxiv.org'}","Deep Graph Contrastive Representation Learning","Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.","arXiv.org","2020","747","['Computer Science', 'Mathematics']","['JournalArticle']","2020-06-07","{'name': 'ArXiv', 'volume': 'abs/2006.04131'}","[{'authorId': '2653121', 'name': 'Yanqiao Zhu'}, {'authorId': '47103911', 'name': 'Yichen Xu'}, {'authorId': '2155385667', 'name': 'Feng Yu'}, {'authorId': '48873756', 'name': 'Q. Liu'}, {'authorId': '50425438', 'name': 'Shu Wu'}, {'authorId': '123865558', 'name': 'Liang Wang'}]"
"aa9ae8096216163ed40dd787917215b5ae4d3d90","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Adversarial Graph Augmentation to Improve Graph Contrastive Learning","Self-supervised learning of graph neural networks (GNN) is in great need because of the widespread label scarcity issue in real-world graph/network data. Graph contrastive learning (GCL), by training GNNs to maximize the correspondence between the representations of the same graph in its different augmented forms, may yield robust and transferable GNNs even without using labels. However, GNNs trained by traditional GCL often risk capturing redundant graph features and thus may be brittle and provide sub-par performance in downstream tasks. Here, we propose a novel principle, termed adversarial-GCL (AD-GCL), which enables GNNs to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in GCL. We pair AD-GCL with theoretical explanations and design a practical instantiation based on trainable edge-dropping graph augmentation. We experimentally validate AD-GCL by comparing with the state-of-the-art GCL methods and achieve performance gains of up-to $14\%$ in unsupervised, $6\%$ in transfer, and $3\%$ in semi-supervised learning settings overall with 18 different benchmark datasets for the tasks of molecule property regression and classification, and social network classification.","Neural Information Processing Systems","2021","310","['Computer Science']","['JournalArticle']","2021-06-10","{'pages': '15920-15933'}","[{'authorId': '31591568', 'name': 'Susheel Suresh'}, {'authorId': '1561672016', 'name': 'Pan Li'}, {'authorId': '145462792', 'name': 'Cong Hao'}, {'authorId': '144050371', 'name': 'Jennifer Neville'}]"
"93ee8e1c05d11d63aa3d61653b2c8bae75e0aecd","{'id': 'bdc2e585-4e48-4e36-8af1-6d859763d405', 'name': 'AAAI Conference on Artificial Intelligence', 'type': 'conference', 'alternate_names': ['National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'url': 'http://www.aaai.org/'}","The Network Data Repository with Interactive Graph Analytics and Visualization","    NetworkRepository (NR) is the first interactive data repository with a web-based platform for visual interactive analytics. Unlike other data repositories (e.g., UCI ML Data Repository, and SNAP), the network data repository (networkrepository.com) allows users to not only download, but to interactively analyze and visualize such data using our web-based interactive graph analytics platform. Users can in real-time analyze, visualize, compare, and explore data along many different dimensions. The aim of NR is to make it easy to discover key insights into the data extremely fast with little effort while also providing a medium for users to share data, visualizations, and insights. Other key factors that differentiate NR from the current data repositories is the number of graph datasets, their size, and variety. While other data repositories are static, they also lack a means for users to collaboratively discuss a particular dataset, corrections, or challenges with using the data for certain applications. In contrast, NR incorporates many social and collaborative aspects that facilitate scientific research, e.g., users can discuss each graph, post observations, and visualizations.   ","AAAI Conference on Artificial Intelligence","2015","2314","['Computer Science']","['JournalArticle', 'Conference']","2015-01-25","{'pages': '4292-4293'}","[{'authorId': '1862090', 'name': 'Ryan A. Rossi'}, {'authorId': '47699955', 'name': 'Nesreen Ahmed'}]"
"3950b578a93674d7a8dc6829581058f76823136e","{'id': '768b87bb-8a18-4d9c-a161-4d483c776bcf', 'name': 'Computer Vision and Pattern Recognition', 'type': 'conference', 'alternate_names': ['CVPR', 'Comput Vis Pattern Recognit'], 'issn': '1063-6919', 'url': 'https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147', 'alternate_urls': ['https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition']}","Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition","Spatial-temporal graphs have been widely used by skeleton-based action recognition algorithms to model human action dynamics. To capture robust movement patterns from these graphs, long-range and multi-scale context aggregation and spatial-temporal dependency modeling are critical aspects of a powerful feature extractor. However, existing methods have limitations in achieving (1) unbiased long-range joint relationship modeling under multi-scale operators and (2) unobstructed cross-spacetime information flow for capturing complex spatial-temporal dependencies. In this work, we present (1) a simple method to disentangle multi-scale graph convolutions and (2) a unified spatial-temporal graph convolutional operator named G3D. The proposed multi-scale aggregation scheme disentangles the importance of nodes in different neighborhoods for effective long-range modeling. The proposed G3D module leverages dense cross-spacetime edges as skip connections for direct information propagation across the spatial-temporal graph. By coupling these proposals, we develop a powerful feature extractor named MS-G3D based on which our model outperforms previous state-of-the-art methods on three large-scale datasets: NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400.","Computer Vision and Pattern Recognition","2020","754","['Computer Science']","['JournalArticle', 'Conference']","2020-03-31","{'name': '2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)', 'pages': '140-149'}","[{'authorId': '2117942226', 'name': 'Ken Ziyu Liu'}, {'authorId': '49724467', 'name': 'Hongwen Zhang'}, {'authorId': '2075098', 'name': 'Zhenghao Chen'}, {'authorId': '2108503703', 'name': 'Zhiyong Wang'}, {'authorId': '3001348', 'name': 'Wanli Ouyang'}]"
"0d67d3ddca1c4e370eaf1e99ec674f612c39c66c","{'id': 'e07422f9-c065-40c3-a37b-75e98dce79fe', 'name': 'The Web Conference', 'type': 'conference', 'alternate_names': ['Web Conf', 'WWW'], 'url': 'http://www.iw3c2.org/'}","Graph Contrastive Learning with Adaptive Augmentation","Recently, contrastive learning (CL) has emerged as a successful method for unsupervised graph representation learning. Most graph CL methods first perform stochastic augmentation on the input graph to obtain two graph views and maximize the agreement of representations in the two views. Despite the prosperous development of graph CL methods, the design of graph augmentation schemes—a crucial component in CL—remains rarely explored. We argue that the data augmentation schemes should preserve intrinsic structures and attributes of graphs, which will force the model to learn representations that are insensitive to perturbation on unimportant nodes and edges. However, most existing methods adopt uniform data augmentation schemes, like uniformly dropping edges and uniformly shuffling features, leading to suboptimal performance. In this paper, we propose a novel graph contrastive representation learning method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Specifically, on the topology level, we design augmentation schemes based on node centrality measures to highlight important connective structures. On the node attribute level, we corrupt node features by adding more noise to unimportant node features, to enforce the model to recognize underlying semantic information. We perform extensive experiments of node classification on a variety of real-world datasets. Experimental results demonstrate that our proposed method consistently outperforms existing state-of-the-art baselines and even surpasses some supervised counterparts, which validates the effectiveness of the proposed contrastive framework with adaptive augmentation.","The Web Conference","2020","1008","['Computer Science']","['JournalArticle', 'Book', 'Conference']","2020-10-27","{'name': 'Proceedings of the Web Conference 2021'}","[{'authorId': '2653121', 'name': 'Yanqiao Zhu'}, {'authorId': '47103911', 'name': 'Yichen Xu'}, {'authorId': '2155385667', 'name': 'Feng Yu'}, {'authorId': '48873756', 'name': 'Q. Liu'}, {'authorId': '50425438', 'name': 'Shu Wu'}, {'authorId': '123865558', 'name': 'Liang Wang'}]"
"8b163b75a6b833911c4e958f8bd52124205382ec","{'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd', 'name': 'Neural Information Processing Systems', 'type': 'conference', 'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'], 'url': 'http://neurips.cc/'}","Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting","Modeling complex spatial and temporal correlations in the correlated time series data is indispensable for understanding the traffic dynamics and predicting the future status of an evolving traffic system. Recent works focus on designing complicated graph neural network architectures to capture shared patterns with the help of pre-defined graphs. In this paper, we argue that learning node-specific patterns is essential for traffic forecasting while the pre-defined graph is avoidable. To this end, we propose two adaptive modules for enhancing Graph Convolutional Network (GCN) with new capabilities: 1) a Node Adaptive Parameter Learning (NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic series automatically. We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations in traffic series automatically based on the two modules and recurrent networks. Our experiments on two real-world traffic datasets show AGCRN outperforms state-of-the-art by a significant margin without pre-defined graphs about spatial connections.","Neural Information Processing Systems","2020","1027","['Computer Science', 'Mathematics']","['JournalArticle']","2020-07-06","{'name': 'ArXiv', 'volume': 'abs/2007.02842'}","[{'authorId': '50010487', 'name': 'Lei Bai'}, {'authorId': '2082966', 'name': 'Lina Yao'}, {'authorId': '2118036284', 'name': 'Can Li'}, {'authorId': '2877263', 'name': 'Xianzhi Wang'}, {'authorId': '2117934428', 'name': 'Can Wang'}]"
"fc3e99ebc07b3014f6736a6a7b077edf2f1634c0","{'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}","GraphSAINT: Graph Sampling Based Inductive Learning Method","Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed this http URL scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the ""neighbor explosion"" problem during minibatch training. Here we proposeGraphSAINT, a graph sampling based inductive learning method that improves training efficiency in a fundamentally different way. By a change of perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling process from the forward and backward propagation of training, and extend GraphSAINT with other graph samplers and GCN variants. Comparing with strong baselines using layer sampling, GraphSAINT demonstrates superior performance in both accuracy and training time on four large graphs.","International Conference on Learning Representations","2019","904","['Computer Science', 'Mathematics']","['JournalArticle']","2019-07-10","{'name': 'ArXiv', 'volume': 'abs/1907.04931'}","[{'authorId': '33352252', 'name': 'Hanqing Zeng'}, {'authorId': '1443735039', 'name': 'Hongkuan Zhou'}, {'authorId': '2215594', 'name': 'Ajitesh Srivastava'}, {'authorId': '2286832947', 'name': 'R. Kannan'}, {'authorId': '1728271', 'name': 'V. Prasanna'}]"
"69381b5efd97e7c55f51c2730caccab3d632d4d2","{'id': '25248f80-fe99-48e5-9b8e-9baef3b8e23b', 'name': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'type': 'journal', 'alternate_names': ['IEEE Trans Pattern Anal Mach Intell'], 'issn': '0162-8828', 'url': 'http://www.computer.org/tpami/', 'alternate_urls': ['http://www.computer.org/portal/web/tpami', 'http://ieeexplore.ieee.org/servlet/opac?punumber=34']}","Graph Embedding and Extensions: A General Framework for Dimensionality Reduction","A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions","IEEE Transactions on Pattern Analysis and Machine Intelligence","2007","2907","['Mathematics', 'Medicine', 'Computer Science']","['JournalArticle']","","{'name': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'pages': '40-51', 'volume': '29'}","[{'authorId': '143653681', 'name': 'Shuicheng Yan'}, {'authorId': '38188040', 'name': 'Dong Xu'}, {'authorId': '2158301', 'name': 'Benyu Zhang'}, {'authorId': '2144973386', 'name': 'HongJiang Zhang'}, {'authorId': '152290618', 'name': 'Qiang Yang'}, {'authorId': '145676588', 'name': 'Stephen Lin'}]"
"150f95f9c73820e0a0fa1546140e9f2bdfd25954","{'id': '1901e811-ee72-4b20-8f7e-de08cd395a10', 'name': 'arXiv.org', 'alternate_names': ['ArXiv'], 'issn': '2331-8422', 'url': 'https://arxiv.org'}","Temporal Graph Networks for Deep Learning on Dynamic Graphs","Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.","arXiv.org","2020","567","['Computer Science', 'Mathematics']","['JournalArticle']","2020-06-18","{'name': 'ArXiv', 'volume': 'abs/2006.10637'}","[{'authorId': '2056294358', 'name': 'Emanuele Rossi'}, {'authorId': '2029302', 'name': 'B. Chamberlain'}, {'authorId': '51484149', 'name': 'Fabrizio Frasca'}, {'authorId': '1775620', 'name': 'D. Eynard'}, {'authorId': '2500309', 'name': 'Federico Monti'}, {'authorId': '1732570', 'name': 'M. Bronstein'}]"
"fdc708aaa0d18c791f878ff2214201410fa52439","","UvA-DARE ( Digital Academic Repository ) Graph Convolutional Matrix Completion","We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.","","2017","1250","","","","","[{'authorId': '9965217', 'name': 'Rianne van den Berg'}, {'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '1678311', 'name': 'M. Welling'}]"
"c7fd29fdd2e0b50a571db4f607eab138e9ecb644","{'id': 'e07422f9-c065-40c3-a37b-75e98dce79fe', 'name': 'The Web Conference', 'type': 'conference', 'alternate_names': ['Web Conf', 'WWW'], 'url': 'http://www.iw3c2.org/'}","MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding","A large number of real-world graphs or networks are inherently heterogeneous, involving a diversity of node types and relation types. Heterogeneous graph embedding is to embed rich structural and semantic information of a heterogeneous graph into low-dimensional node representations. Existing models usually define multiple metapaths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models either omit node content features, discard intermediate nodes along the metapath, or only consider one metapath. To address these three limitations, we propose a new model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the final performance. Specifically, MAGNN employs three major components, i.e., the node content transformation to encapsulate input node attributes, the intra-metapath aggregation to incorporate intermediate semantic nodes, and the inter-metapath aggregation to combine messages from multiple metapaths. Extensive experiments on three real-world heterogeneous graph datasets for node classification, node clustering, and link prediction show that MAGNN achieves more accurate prediction results than state-of-the-art baselines.","The Web Conference","2020","782","['Computer Science']","['Book', 'JournalArticle', 'Conference']","2020-02-05","{'name': 'Proceedings of The Web Conference 2020'}","[{'authorId': '15473346', 'name': 'Xinyu Fu'}, {'authorId': '73329314', 'name': 'Jiani Zhang'}, {'authorId': '1491449344', 'name': 'Ziqiao Meng'}, {'authorId': '145310663', 'name': 'Irwin King'}]"
"4f08a18f205e41c786fc80160ce8e133d50832dc","","On the shortest spanning subtree of a graph and the traveling salesman problem","7. A. Kurosh, Ringtheoretische Probleme die mit dem Burnsideschen Problem uber periodische Gruppen in Zussammenhang stehen, Bull. Acad. Sei. URSS, Ser. Math. vol. 5 (1941) pp. 233-240. 8. J. Levitzki, On the radical of a general ring, Bull. Amer. Math. Soc. vol. 49 (1943) pp. 462^66. 9. -, On three problems concerning nil rings, Bull. Amer. Math. Soc. vol. 49 (1943) pp. 913-919. 10. -, On the structure of algebraic algebras and related rings, Trans. Amer. Math. Soc. vol. 74 (1953) pp. 384-409.","","1956","5274","['Mathematics']","","","{'name': '', 'pages': '48-50', 'volume': '7'}","[{'authorId': '10398168', 'name': 'J. Kruskal'}]"
"006906b6bbe5c1f378cde9fd86de1ce9e6b131da","{'id': 'c6840156-ee10-4d78-8832-7f8909811576', 'name': 'IEEE Transactions on Knowledge and Data Engineering', 'type': 'journal', 'alternate_names': ['IEEE Trans Knowl Data Eng'], 'issn': '1041-4347', 'url': 'https://www.computer.org/web/tkde', 'alternate_urls': ['http://ieeexplore.ieee.org/servlet/opac?punumber=69']}","A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications","Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios.","IEEE Transactions on Knowledge and Data Engineering","2017","1735","['Computer Science']","['JournalArticle', 'Review']","2017-09-22","{'name': 'IEEE Transactions on Knowledge and Data Engineering', 'pages': '1616-1637', 'volume': '30'}","[{'authorId': '143754377', 'name': 'Hongyun Cai'}, {'authorId': '3113725', 'name': 'V. Zheng'}, {'authorId': '143922493', 'name': 'K. Chang'}]"
"0da8af8d81e84381ffe656a0bbf2f3937ffac618","","Neural Motifs: Scene Graph Parsing with Global Context","We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also find that there are recurring patterns even in larger subgraphs: more than 50% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1% relative gain. Our code is available at github.com/rowanz/neural-motifs.","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","2017","935","['Computer Science']","['JournalArticle', 'Conference']","2017-11-17","{'name': '2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition', 'pages': '5831-5840'}","[{'authorId': '2545335', 'name': 'Rowan Zellers'}, {'authorId': '2064210', 'name': 'Mark Yatskar'}, {'authorId': '38094552', 'name': 'Sam Thomson'}, {'authorId': '1699545', 'name': 'Yejin Choi'}]"
"91fb815361fdbf80ff15ce4d783a41846bd99232","{'id': 'a0edb93b-1e95-4128-a295-6b1659149cef', 'name': 'Knowledge Discovery and Data Mining', 'type': 'conference', 'alternate_names': ['KDD', 'Knowl Discov Data Min'], 'url': 'http://www.acm.org/sigkdd/'}","GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training","Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) --- a self-supervised graph neural network pre-training framework --- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.","Knowledge Discovery and Data Mining","2020","897","['Computer Science', 'Mathematics']","['JournalArticle', 'Book', 'Conference']","2020-06-17","{'name': 'Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining'}","[{'authorId': '40125294', 'name': 'J. Qiu'}, {'authorId': '50282546', 'name': 'Qibin Chen'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '39772285', 'name': 'Jing Zhang'}, {'authorId': '38385080', 'name': 'Hongxia Yang'}, {'authorId': '2055623340', 'name': 'Ming Ding'}, {'authorId': '1748169', 'name': 'Kuansan Wang'}, {'authorId': '46199760', 'name': 'Jie Tang'}]"
"6c44f8e62d824bcda4f291c679a5518bbd4225f6","{'id': 'a0edb93b-1e95-4128-a295-6b1659149cef', 'name': 'Knowledge Discovery and Data Mining', 'type': 'conference', 'alternate_names': ['KDD', 'Knowl Discov Data Min'], 'url': 'http://www.acm.org/sigkdd/'}","Adversarial Attacks on Neural Networks for Graph Data","Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model.We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.","Knowledge Discovery and Data Mining","2018","1021","['Computer Science', 'Mathematics']","['JournalArticle', 'Book', 'Conference']","2018-05-21","{'name': 'Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining'}","[{'authorId': '3156540', 'name': 'Daniel Zügner'}, {'authorId': '46256784', 'name': 'Amir Akbarnejad'}, {'authorId': '3075189', 'name': 'Stephan Günnemann'}]"
